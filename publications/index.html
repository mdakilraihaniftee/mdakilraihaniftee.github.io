<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Md Akil Raihan Iftee </title> <meta name="author" content="Md Akil Raihan Iftee"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg?f7d203edd0de5f513cb08f8e1ffe2533"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mdakilraihaniftee.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Md <span class="font-weight-bold"> Akil </span> Raihan Iftee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item"> <a class="nav-link" href="/#education"> Education </a> </li> <li class="nav-item"> <a class="nav-link" href="/#workexperience"> Experience </a> </li> <li class="nav-item"> <a class="nav-link" href="/#honowards"> Achievements </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/slowmo-480.webp 480w,/assets/img/publication_preview/slowmo-800.webp 800w,/assets/img/publication_preview/slowmo-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/slowmo.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="slowmo.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2024slowmo" class="col-sm-8"> <div class="title">SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation</div> <div class="author"> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://mdakilraihaniftee.github.io/assets/pdf/slomofast.pdf" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/slomofast.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework efficiently preserves knowledge of past domains, adapts efficiently to new ones. Our extensive experimental results demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across CTTA benchmarks, achieving a mean error rate of 33.8% in various TTA settings. Notably, it surpasses existing methods by a margin of at least 1.5%. Additionally, SloMo-Fast achieves significant performance improvements in Mixed Domain and our proposed new benchmark Mixed domain comes after Continual Domain scenarios along with Cyclic repeatation in continual test time adaptation setting, indicating its ability to learn generalized representations across domains.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fedbalancetta-480.webp 480w,/assets/img/publication_preview/fedbalancetta-800.webp 800w,/assets/img/publication_preview/fedbalancetta-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/fedbalancetta.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fedbalancetta.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2025pfedbbn" class="col-sm-8"> <div class="title">pFedBBN: A Personalized Federated Test-Time Adaptation with Balanced Batch Normalization for Class-Imbalanced Data</div> <div class="author"> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://drive.google.com/file/d/19IwL73UUH9O9aSI1rPGhJGq6p7ve1jkO/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://drive.google.com/file/d/19IwL73UUH9O9aSI1rPGhJGq6p7ve1jkO/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Federated learning (FL) enables collaborative model training across decentralized clients while preserving the privacy of local data. Class imbalance remains a fundamental challenge in FL, where rare but critical classes are often severely underrepresented in individual client datasets. Although prior work has addressed class imbalance during training through reliable aggregation and local class distribution alignment, these methods typically rely on access to labeled data or coordination among clients, and none are designed for unsupervised adaptation during inference under federated constraints. We propose a new approach called pFedBBN, a personalized federated test-time adaptation framework that incorporates balanced batch normalization (BBN) to mitigate prediction bias by treating all classes equally during feature normalization, while also enabling a weighted collaboration of clients based on their BBN statistics, which indicates that clients from similar domains prioritize each other more during adaptation. The method supports fully unsupervised local adaptation and introduces a class-aware model aggregation strategy that enables personalized inference without compromising privacy. It effectively addresses both distribution shifts and class imbalance through balanced feature normalization and domain-aware collaboration, without requiring any labeled or raw data from clients. Experiments on corrupted versions of CIFAR-10 and CIFAR-100 demonstrate that pFedBBN significantly improves robustness and performance on minority classes compared to existing federated and test-time adaptation baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Accepted</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fedctta-480.webp 480w,/assets/img/publication_preview/fedctta-800.webp 800w,/assets/img/publication_preview/fedctta-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/fedctta.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fedctta.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rajib2024fedctta" class="col-sm-8"> <div class="title">FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning</div> <div class="author"> </div> <div class="periodical"> <em>In 2025 International Joint Conference on Neural Networks (IJCNN-2025)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://mdakilraihaniftee.github.io/assets/pdf/FedCTTA.pdf" class="btn btn-sm z-depth-0" role="button">HTML</a> <a href="/assets/pdf/FedCTTA.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it ideal for privacy-sensitive applications. However, FL models often suffer performance degradation due to distribution shifts between training and deployment. Test-Time Adaptation (TTA) offers a promising solution by allowing models to adapt using only test samples. However, existing TTA methods in FL face challenges such as computational overhead, privacy risks from feature sharing, and scalability concerns due to memory constraints. To address these limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a privacy-preserving and computationally efficient framework for federated adaptation. Unlike prior methods that rely on sharing local feature statistics, FedCTTA avoids direct feature exchange by leveraging similarity-aware aggregation based on model output distributions over randomly generated noise samples. This approach ensures adaptive knowledge sharing while preserving data privacy. Furthermore, FedCTTA minimizes the entropy at each client for continual adaptation, enhancing the model’s confidence in evolving target distributions. Our method eliminates the need for server-side training during adaptation and maintains a constant memory footprint, making it scalable even as the number of clients or training rounds increases. Extensive experiments show that FedCTTA surpasses existing methods across diverse temporal and spatial heterogeneity scenarios.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Accepted</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lulc-480.webp 480w,/assets/img/publication_preview/lulc-800.webp 800w,/assets/img/publication_preview/lulc-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/lulc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lulc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="HOSSAIN2025LULC" class="col-sm-8"> <div class="title">BD Open LULC Map: High-Resolution Land Use and Land Cover Mapping &amp; Benchmarking for Urban Development in Dhaka, Bangladesh</div> <div class="author"> Mir Sazzat Hossain, Ovi Paul, Md Akil Raihan Iftee, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Rakibul Hasan Rajib, Abu Bakar Siddik Nayem, Anis Sarker, Arshad Momen, Md Ashraful Amin, Amin Ahsan Ali, AKM Mahbubur Rahman' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>In 2025 IEEE International Conference on Image Processing (ICIP)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2505.21915" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/abs/2505.21915" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We introduce a novel machine learning dataset tailored for the classification of bent radio active galactic nuclei (AGN) in astronomical observations. Bent radio AGN, distinguished by their curved jet structures, provide critical insights into galaxy cluster dynamics, interactions within the intracluster medium, and the broader physics of AGN. Despite their astrophysical significance, the classification of bent AGN remains a challenge due to the scarcity of specialized datasets and benchmarks. To address this, we present a dataset derived from a well recognized radio astronomy survey, designed to support the classification of NAT (Narrow-Angle Tail) and WAT (Wide-Angle Tail) categories, along with detailed data processing steps. We further evaluate the performance of stateof-the-art deep learning models on the dataset, including Convolutional Neural Networks (CNNs) and transformer-based architectures. Our results demonstrate the effectiveness of advanced machine learning models in classifying bent radio AGN, with ConvNeXT achieving the highest F1-scores for both NAT and WAT sources. By sharing this dataset and benchmarks, we aim to facilitate the advancement of research in bent AGN classification, AGN and cluster environments, galaxy evolution, and more.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Accepted</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rgc-480.webp 480w,/assets/img/publication_preview/rgc-800.webp 800w,/assets/img/publication_preview/rgc-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/rgc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rgc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="HOSSAIN2023512" class="col-sm-8"> <div class="title">RGC-BENT: A NOVEL DATASET FOR BENT RADIO GALAXY CLASSIFICATION</div> <div class="author"> M.S. Hossain, K.M.B. Asad, P. Saikia, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'A. Khan, M.A.R Iftee, A. Momen, A.A. Ali, J.K. Ghosh, M.A. Amin, A.K.M.M. Rahman' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>In 2025 IEEE International Conference on Image Processing (ICIP)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2505.19249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2505.19249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We introduce a novel machine learning dataset tailored for the classification of bent radio active galactic nuclei (AGN) in astronomical observations. Bent radio AGN, distinguished by their curved jet structures, provide critical insights into galaxy cluster dynamics, interactions within the intracluster medium, and the broader physics of AGN. Despite their astrophysical significance, the classification of bent AGN remains a challenge due to the scarcity of specialized datasets and benchmarks. To address this, we present a dataset derived from a well recognized radio astronomy survey, designed to support the classification of NAT (Narrow-Angle Tail) and WAT (Wide-Angle Tail) categories, along with detailed data processing steps. We further evaluate the performance of stateof-the-art deep learning models on the dataset, including Convolutional Neural Networks (CNNs) and transformer-based architectures. Our results demonstrate the effectiveness of advanced machine learning models in classifying bent radio AGN, with ConvNeXT achieving the highest F1-scores for both NAT and WAT sources. By sharing this dataset and benchmarks, we aim to facilitate the advancement of research in bent AGN classification, AGN and cluster environments, galaxy evolution, and more.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Published</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/moe-480.webp 480w,/assets/img/publication_preview/moe-800.webp 800w,/assets/img/publication_preview/moe-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/moe.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="moe.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2024moe-tta" class="col-sm-8"> <div class="title">MoE-TTA: Enhancing Continual Test-Time Adaptation for Vision-Language Models through Mixture of Experts</div> <div class="author"> Md Akil Raihan Iftee, Wahida Mahjabin, Anik Ekka, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sunanda Das' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2024 27th International Conference on Computer and Information Technology (ICCIT)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/11022003" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://mdakilraihaniftee.github.io/assets/pdf/moe-tta.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Continual learning enables vision-language models to incrementally acquire new knowledge without relying on access to the entire historical dataset. This capability is crucial for adapting to evolving data distributions in real-world scenar- ios, where models must handle domain shifts and incorporate new information while retaining previously learned knowledge. However, maintaining performance in large-scale models remains challenging due to parameter shifts during learning and the sig- nificant computational costs associated with full-model updates. To address these challenges, this paper introduces a novel method for Continual Test-Time Domain Adaptation (CTTDA) on vision- language datasets, leveraging a MoE and adapter modules to optimize domain-specific adaptation while maintaining zero-shot classification capabilities. Utilising LoRA in MoE framework within transformer layers, the model efficiently updates a small set of parameters by dynamically selecting experts. This approach minimizes computational costs by activating only a portion of the model, avoiding the need for full-model updates during domain adaptation. During test-time adaptation, entropy loss is calculated without access to labels, improving the model’s fine-tuning and guiding towards confident predictions across domains. A contrastive warm-up phase further optimizes the adapter blocks by enhancing the differentiation of domain-specific and domain- invariant features, thereby establishing a strong foundation for effective test-time adaptation. The proposed MoE-TTA model achieves an average accuracy of 36.43% across diverse ImageNet datasets and 32.93% in fine-grained classification, demonstrating promising results, especially in datasets like EuroSAT (51.67%), while being lower than several competitors, with no doubt in its ability to capture domain shifts effectively.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Published</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/llavaseg-480.webp 480w,/assets/img/publication_preview/llavaseg-800.webp 800w,/assets/img/publication_preview/llavaseg-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/llavaseg.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llavaseg.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2024organ-seg" class="col-sm-8"> <div class="title"> Organ-Seg: A Vision-Language and LLM-Enhanced Framework for User-Guided Abdominal Organ Image Segmentation</div> <div class="author"> Md Akil Raihan Iftee, Tasfia Faija, Partho Choudhury Shoumya, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Sunanda Das' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2024 27th International Conference on Computer and Information Technology (ICCIT)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/11022489" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://mdakilraihaniftee.github.io/assets/pdf/organ-seg.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Medical image segmentation plays a vital role in diagnostic and treatment planning, where precision is crucial for accurate outcomes. Traditional segmentation methods, while effective in many areas, often fail to incorporate user-driven guidance, leading to errors in region identification, especially when irrelevant regions are segmented. In this study, we present a new, instruction-based medical image segmentation framework that enhances user interaction while delivering precise and context-aware results. Our approach addresses the limitations of previous works, such as vision-large language models (LLM) like LLaVA, which provide context but do not perform segmentation, and the Segment Anything Model, which performs segmenta- tion but does not incorporate user’s text-guided instruction. We propose a segmentation model framework that combines vision-language embeddings from LLava with SAM to perform accurate, query-based segmentation of medical images. A key innovation of our model framework is its ability to handle false premises—situations where a user queries for an organ not present in the image—by employing a similarity-based mechanism that prevents incorrect segmentation. Tested on MRI datasets, FLARE22, our system achieves the highest segmentation dice coefficient 63.9%, with significantly improved relevance and reliability. The results demonstrate the effectiveness of our approach in refining segmentation quality and enhancing user- guided interaction, thus offering an advanced tool for medical imaging applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Published</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/nextgen-480.webp 480w,/assets/img/publication_preview/nextgen-800.webp 800w,/assets/img/publication_preview/nextgen-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/nextgen.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="nextgen.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2024next-gen" class="col-sm-8"> <div class="title">Next-Gen Heart Disease Prediction: A Gradient Boosting Approach with L1-Regularized Neural Networks</div> <div class="author"> Md Akil Raihan Iftee, Raiyan Ashraf, Atiqul Islam, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Shahana Akter, Sunanda Das' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 2024 27th International Conference on Computer and Information Technology (ICCIT)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/11022378" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/next-gen_.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Medical image segmentation plays a vital role in diagnostic and treatment planning, where precision is crucial for accurate outcomes. Traditional segmentation methods, while effective in many areas, often fail to incorporate user-driven guidance, leading to errors in region identification, especially when irrelevant regions are segmented. In this study, we present a new, instruction-based medical image segmentation framework that enhances user interaction while delivering precise and context-aware results. Our approach addresses the limitations of previous works, such as vision-large language models (LLM) like LLaVA, which provide context but do not perform segmentation, and the Segment Anything Model, which performs segmenta- tion but does not incorporate user’s text-guided instruction. We propose a segmentation model framework that combines vision-language embeddings from LLava with SAM to perform accurate, query-based segmentation of medical images. A key innovation of our model framework is its ability to handle false premises—situations where a user queries for an organ not present in the image—by employing a similarity-based mechanism that prevents incorrect segmentation. Tested on MRI datasets, FLARE22, our system achieves the highest segmentation dice coefficient 63.9%, with significantly improved relevance and reliability. The results demonstrate the effectiveness of our approach in refining segmentation quality and enhancing user- guided interaction, thus offering an advanced tool for medical imaging applications.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Thesis Book</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/thesis-480.webp 480w,/assets/img/publication_preview/thesis-800.webp 800w,/assets/img/publication_preview/thesis-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/thesis.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="thesis.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2023thesis" class="col-sm-8"> <div class="title">3D Cerebrovascular Segmentation Using Semi-Supervised Approach</div> <div class="author"> Sk. Imran Hossain MD Akil Raihan Iftee </div> <div class="periodical"> <em></em> 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/thesis_report.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Published</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/vionet_vid-480.webp 480w,/assets/img/publication_preview/vionet_vid-800.webp 800w,/assets/img/publication_preview/vionet_vid-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/vionet_vid.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vionet_vid.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2023vionet" class="col-sm-8"> <div class="title">VioNet: An Enhanced Violence Detection Approach for Videos Using a Fusion Model of Vision Transformer with Bi-LSTM and 3D Convolutional Neural Networks</div> <div class="author"> Md Akil Raihan Iftee, Md Mominur Rahman, and Sunanda Das </div> <div class="periodical"> <em>In International Conference on Big Data, IoT and Machine Learning</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-981-99-8937-9_10" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Violence_detection.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The identification of violence in real-world scenarios is imperative as it enables the detection of aggressive behavior, thereby preventing harm to individuals and communities. This is crucial for ensuring public safety and maintaining social order.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Published</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/unsup_heart-480.webp 480w,/assets/img/publication_preview/unsup_heart-800.webp 800w,/assets/img/publication_preview/unsup_heart-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/unsup_heart.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="unsup_heart.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="iftee2023unsupervised" class="col-sm-8"> <div class="title">Unsupervised Binary Classification of Heart Diseases Using an Autoencoder Model with Boosting Algorithm</div> <div class="author"> Md Akil Raihan Iftee, and Sunanda Das </div> <div class="periodical"> <em>In 2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/10306451" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/unsup.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The identification of violence in real-world scenarios is imperative as it enables the detection of aggressive behavior, thereby preventing harm to individuals and communities. This is crucial for ensuring public safety, facilitating effective crime investigation, promoting child safety, safeguarding mental health, and facilitating social media moderation. Various methods, including handcrafted techniques and deep learning algorithms, can be utilized in surveillance or CCTV cameras, as well as smartphones, to enable timely detection of violent behavior and facilitate appropriate action and intervention. In this study, we introduce VioNET, a novel approach that combines a 3D Convolutional Neural Network and a Vision Transformer with Bidirectional LSTM for the purpose of accurately detecting violence in video data. Since video data is inherently sequential, the extraction of spatiotemporal features is essential to accurate detection. The use of these two deep learning methods facilitates the extraction of maximum features, which are then fused together to classify videos with the highest possible accuracy. We evaluate the effectiveness of our approach by employing three datasets: Hockey, Movies, and Violent Flow, for analysis. The proposed model achieved impressive accuracies of 97.85%, 100.00%, and 96.33% on the Hokey, Movie, and Violent Flow datasets, respectively. Based on the obtained results, it is evident that our method showcases superior performance, outperforming several existing approaches in the field and establishing itself as a robust and competitive solution for violence detection in videos.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Published</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bangla_nlp-480.webp 480w,/assets/img/publication_preview/bangla_nlp-800.webp 800w,/assets/img/publication_preview/bangla_nlp-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/bangla_nlp.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bangla_nlp.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sen2022bangla" class="col-sm-8"> <div class="title">Bangla natural language processing: A comprehensive analysis of classical, machine learning, and deep learning-based methods</div> <div class="author"> Ovishake Sen, Mohtasim Fuad, Md Nazrul Islam, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Jakaria Rabbi, Mehedi Masud, Md Kamrul Hasan, Md Abdul Awal, Awal Ahmed Fime, Md Tahmid Hasan Fuad, Delowar Sikder, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>IEEE Access</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9751052" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9751052" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The Bangla language is the seventh most spoken language, with 265 million native and non-native speakers worldwide. However, English is the predominant language for online resources and...</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Published</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/face_recognition-480.webp 480w,/assets/img/publication_preview/face_recognition-800.webp 800w,/assets/img/publication_preview/face_recognition-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/face_recognition.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="face_recognition.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="fuad2021recent" class="col-sm-8"> <div class="title">Recent advances in deep learning techniques for face recognition</div> <div class="author"> Md Tahmid Hasan Fuad, Awal Ahmed Fime, Delowar Sikder, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Md Akil Raihan Iftee, Jakaria Rabbi, Mabrook S Al-Rakhami, Abdu Gumaei, Ovishake Sen, Mohtasim Fuad' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>IEEE Access</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9478893" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9478893" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In recent years, researchers have proposed many deep learning (DL) methods for various tasks, and particularly face recognition (FR) made an enormous leap using these techniques. Deep FR systems benefit from the hierarchical architecture of the DL methods to learn discriminative face representation. Therefore, DL techniques significantly improve state-of-the-art performance on FR systems and encourage diverse and efficient real-world applications. In this paper, we present a comprehensive analysis of various FR systems that leverage the different types of DL techniques, and for the study, we summarize 171 recent contributions from this area. We discuss the papers related to different algorithms, architectures, loss functions, activation functions, datasets, challenges, improvement ideas, current and future trends of DL-based FR systems. We provide a detailed discussion of various DL methods to understand the current state-of-the-art, and then we discuss various activation and loss functions for the methods. Additionally, we summarize different datasets used widely for FR tasks and discuss challenges related to illumination, expression, pose variations, and occlusion. Finally, we discuss improvement ideas, current and future trends of FR tasks.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Md Akil Raihan Iftee. Last updated: November 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-P1606ZLN4B"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-P1606ZLN4B");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-teaching",title:"Teaching",description:"Welcome to my teaching portfolio! Here you&#39;ll find an overview of the courses and tutorial sessions I&#39;ve taught, organized by semester and topic. I regularly update this page with slide links and materials for students and collaborators.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-machine-unlearning-in-mllms",title:"Machine Unlearning in MLLMs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/mu-vlm/"}},{id:"post-federated-unlearning-and-it-39-s-adversarial-threats",title:"Federated Unlearning and it&#39;s Adversarial Threats",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/federated-unlearning/"}},{id:"post-machine-unlearning-and-it-39-s-adversarial-threats",title:"Machine Unlearning and it&#39;s Adversarial Threats",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/machine-unlearning/"}},{id:"post-adversarial-attacks-and-threats-in-federated-learning",title:"Adversarial Attacks and Threats in Federated Learning",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/federated_adversarial_attack/"}},{id:"post-personal-blog-different-losses-of-test-time-adaptation-methods",title:"Personal Blog: Different Losses of Test Time Adaptation Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/losses_of_tta/"}},{id:"post-paper-review-adversarial-attacks-in-test-time-adaptation-methods",title:"Paper Review - Adversarial Attacks in Test Time Adaptation Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/presentation_ccds_attacks_in_da/"}},{id:"news-successfully-defended-undergraduate-thesis-under-the-supervision-of-sk-imran-hossain",title:"Successfully defended undergraduate thesis under the supervision of Sk. Imran Hossain",description:"",section:"News"},{id:"news-started-an-internship-at-the-center-for-computational-amp-amp-data-sciences",title:"Started an internship at the Center for Computational &amp;amp;amp; Data Sciences",description:"",section:"News"},{id:"news-i-have-joined-as-a-research-assistant-at-center-for-computational-amp-amp-data-sciences",title:"I have joined as a Research Assistant at Center for Computational &amp;amp;amp; Data...",description:"",section:"News"},{id:"news-i-am-excited-to-share-that-3-of-my-papers-have-been-accepted-for-presentation-at-the-27th-international-conference-on-computer-and-information-technology-iccit-2024",title:"I am excited to share that 3 of my papers have been accepted...",description:"",section:"News"},{id:"news-our-fedctta-paper-got-accepted-in-ijcnn-2025",title:"Our FedCTTA paper got accepted in  IJCNN 2025",description:"",section:"News"},{id:"news-two-of-our-papers-bd-open-lulc-map-and-rgc-bent-have-been-accepted-in-ieee-international-conference-on-image-processing-icip-2025",title:"Two of our papers BD Open LULC Map and RGC-BENT have been accepted...",description:"",section:"News"},{id:"news-reaching-200-citations-of-my-research-papers-check-in-my-google-scholar",title:"Reaching 200+ Citations of my research papers. Check in my Google Scholar.",description:"",section:"News"},{id:"news-our-slomo-fast-fedpoisonttp-and-pfedbbn-are-avialable-in-online-please-have-a-look-at-the-papers",title:"Our SloMo-Fast, FedPoisonTTP and pFedBBN are avialable in online. Please have a look...",description:"",section:"News"},{id:"projects-human-activity-recognition-through-wearable-sensor-video",title:"Human Activity Recognition through Wearable Sensor, Video",description:"Multimodal Alignment from Video, Sensor(accelerometer, gyroscope, and orientation), Language",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-fedbalancetta",title:"FedBalanceTTA",description:"Class Imbalance Mitigation Federated Test-Time Adaptation",section:"Projects",handler:()=>{window.location.href="/projects/1_project_1/"}},{id:"projects-user-guided-image-editing",title:"User-Guided Image Editing",description:"Diffusion-Based Image Editing with Vision-Language Instructions",section:"Projects",handler:()=>{window.location.href="/projects/1_project_2/"}},{id:"projects-semantic-cognitive-distraction-attack-scd-in-llm",title:"Semantic Cognitive Distraction Attack (SCD) in LLM",description:"Multimodal Jailbreak Attack via Contextual Reasoning Manipulation",section:"Projects",handler:()=>{window.location.href="/projects/1_project_3/"}},{id:"projects-corona-app",title:"Corona App",description:"Real-time COVID-19 Tracker App Using Flutter &amp; REST API",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-ludo-2-0",title:"Ludo 2.0",description:"Python-Based Human vs AI Board Game",section:"Projects",handler:()=>{window.location.href="/projects/2_project_1/"}},{id:"projects-online-market",title:"Online Market",description:"ASP.NET Core MVC-based E-commerce Platform",section:"Projects",handler:()=>{window.location.href="/projects/2_project_2/"}},{id:"projects-video-streaming-dbms",title:"Video Streaming DBMS",description:"Oracle SQL-based Database System for a YouTube-like Platform",section:"Projects",handler:()=>{window.location.href="/projects/2_project_3/"}},{id:"projects-federated-personalized-scanpath",title:"Federated Personalized Scanpath",description:"Federated Learning",section:"Projects",handler:()=>{window.location.href="/projects/2_project_5/"}},{id:"projects-controllable-3d-user-interface-generation",title:"Controllable 3D User Interface Generation",description:"Multimodal Learning, 3D, Generative AI",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-federated-personalized-eye-scanpath",title:"Federated Personalized Eye Scanpath",description:"Federated Learning + Trustworthy ML",section:"Projects",handler:()=>{window.location.href="/projects/3_project_1/"}},{id:"projects-gaze-enhanced-multimodal-interaction",title:"Gaze-Enhanced Multimodal Interaction",description:"Multimodal Learning",section:"Projects",handler:()=>{window.location.href="/projects/3_project_2/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%69%66%74%65%65%31%38%30%37%30%30%32@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/mdakilraihaniftee","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/mdakilraihaniftee","_blank")}},{id:"social-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Akil Raihan Iftee/","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=x1ueJ5UAAAAJ&hl","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>
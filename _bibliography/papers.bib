@article{fuad2021recent,
  title={Recent advances in deep learning techniques for face recognition},
  author={Fuad, Md Tahmid Hasan and Fime, Awal Ahmed and Sikder, Delowar and Iftee, Md Akil Raihan and Rabbi, Jakaria and Al-Rakhami, Mabrook S and Gumaei, Abdu and Sen, Ovishake and Fuad, Mohtasim},
  journal={IEEE Access},
  volume={9},
  pages={99112--99142},
  year={2021},
  publisher={IEEE},
  abstract={In recent years, researchers have proposed many deep learning (DL) methods for various tasks, and particularly face recognition (FR) made an enormous leap using these techniques. Deep FR systems benefit from the hierarchical architecture of the DL methods to learn discriminative face representation. Therefore, DL techniques significantly improve state-of-the-art performance on FR systems and encourage diverse and efficient real-world applications. In this paper, we present a comprehensive analysis of various FR systems that leverage the different types of DL techniques, and for the study, we summarize 171 recent contributions from this area. We discuss the papers related to different algorithms, architectures, loss functions, activation functions, datasets, challenges, improvement ideas, current and future trends of DL-based FR systems. We provide a detailed discussion of various DL methods to understand the current state-of-the-art, and then we discuss various activation and loss functions for the methods. Additionally, we summarize different datasets used widely for FR tasks and discuss challenges related to illumination, expression, pose variations, and occlusion. Finally, we discuss improvement ideas, current and future trends of FR tasks.},
  html={https://ieeexplore.ieee.org/abstract/document/9478893},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9478893},
  abbr={Published},
  preview={face_recognition.jpg}
}

@article{sen2022bangla,
  title={Bangla natural language processing: A comprehensive analysis of classical, machine learning, and deep learning-based methods},
  author={Sen, Ovishake and Fuad, Mohtasim and Islam, Md Nazrul and Rabbi, Jakaria and Masud, Mehedi and Hasan, Md Kamrul and Awal, Md Abdul and Fime, Awal Ahmed and Fuad, Md Tahmid Hasan and Sikder, Delowar and others},
  journal={IEEE Access},
  volume={10},
  pages={38999--39044},
  year={2022},
  publisher={IEEE},
  abstract={The Bangla language is the seventh most spoken language, with 265 million native and non-native speakers worldwide. However, English is the predominant language for online resources and...},
  html={https://ieeexplore.ieee.org/abstract/document/9751052},
  pdf={https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9751052},
  abbr={Published},
  preview={bangla_nlp.jpg}
}

@misc{iftee2023thesis,
  title={3D Cerebrovascular Segmentation Using Semi-Supervised Approach},
  author={MD Akil Raihan Iftee, Sunanda Das, Sk. Imran Hossain},
  booktitle={Undergraduate Thesis paper},
  pages={},
  year={2023},
  publisher={},
  pdf={thesis_report.pdf},
  abbr={Thesis Book},
  preview={thesis.jpg}
}

@inproceedings{iftee2023vionet,
  title={VioNet: An Enhanced Violence Detection Approach for Videos Using a Fusion Model of Vision Transformer with Bi-LSTM and 3D Convolutional Neural Networks},
  author={Iftee, Md Akil Raihan and Rahman, Md Mominur and Das, Sunanda},
  booktitle={International Conference on Big Data, IoT and Machine Learning},
  pages={139--151},
  year={2023},
  organization={Springer},
  abstract={The identification of violence in real-world scenarios is imperative as it enables the detection of aggressive behavior, thereby preventing harm to individuals and communities. This is crucial for ensuring public safety and maintaining social order.},
  html={https://link.springer.com/chapter/10.1007/978-981-99-8937-9_10},
  pdf={Violence_detection.pdf},
  abbr={Published},
  preview={vionet_vid.png}
}
@inproceedings{iftee2023unsupervised,
  title={Unsupervised Binary Classification of Heart Diseases Using an Autoencoder Model with Boosting Algorithm},
  author={Iftee, Md Akil Raihan and Das, Sunanda},
  booktitle={2023 14th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
  pages={1--7},
  year={2023},
  organization={IEEE},
  abstract={The identification of violence in real-world scenarios is imperative as it enables the detection of aggressive behavior, thereby preventing harm to individuals and communities. This is crucial for ensuring public safety, facilitating effective crime investigation, promoting child safety, safeguarding mental health, and facilitating social media moderation. Various methods, including handcrafted techniques and deep learning algorithms, can be utilized in surveillance or CCTV cameras, as well as smartphones, to enable timely detection of violent behavior and facilitate appropriate action and intervention. In this study, we introduce VioNET, a novel approach that combines a 3D Convolutional Neural Network and a Vision Transformer with Bidirectional LSTM for the purpose of accurately detecting violence in video data. Since video data is inherently sequential, the extraction of spatiotemporal features is essential to accurate detection. The use of these two deep learning methods facilitates the extraction of maximum features, which are then fused together to classify videos with the highest possible accuracy. We evaluate the effectiveness of our approach by employing three datasets: Hockey, Movies, and Violent Flow, for analysis. The proposed model achieved impressive accuracies of 97.85%, 100.00%, and 96.33% on the Hokey, Movie, and Violent Flow datasets, respectively. Based on the obtained results, it is evident that our method showcases superior performance, outperforming several existing approaches in the field and establishing itself as a robust and competitive solution for violence detection in videos.},
  html={https://ieeexplore.ieee.org/abstract/document/10306451},
  pdf={unsup.pdf},
  abbr={Published},
  preview={unsup_heart.png}
}

@misc{iftee2024slowmo,
  title={SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation},
  author={MD Akil Raihan Iftee, Mir Sazzat Hossain, Rakibul Rajib, A K M Mahbubur, Tariq Iqbal, Md Mofijul, M Ashraful, and Amin Ahsan Ali},
  booktitle={Under Review CVPR-2025},
  pages={},
  year={2024},
  publisher={IEEE},
  abstract={Continual Test-Time Adaptation (CTTA) is crucial for de- ploying models in real-world applications with unseen, evolving target domains and restricted access to source data. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy- sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which de- grades performance on previously encountered domains as target domains shift. To address these challenges, we pro- pose SloMo-Fast, a source-free, dual-teacher CTTA frame- work designed for enhanced adaptability and generaliza- tion. SloMo-Fast includes two complementary teachers: the Slow-Teacher, which adapts gradually to ensure ro- bust generalization by tracking an exponential moving av- erage of the student’s parameters, and the Fast-Teacher, which quickly adapts to new domains by constructing proto- types from high-confidence test samples, gathering knowl- edge across domains. This efficiently approach preserves knowledge of past domains, adapts efficiently to new ones, and reduces computational complexity by relying solely on batch normalization updates. SloMo-Fast consistently outperforms state-of-the-art methods across CTTA bench- marks, achieving mean error rates of 15.79% on CIFAR10- C, 27.01% on CIFAR100-C, and 54.2% on ImageNet-C. It also demonstrates strong performance on Mixed Domain and our proposed benchmark with cyclic repetition in the continual TTA setting, highlighting its ability to learn gen- eralized representations across domains.},
  pdf={https://drive.google.com/file/d/1ZqOfTNb33Kgs9NSKmrQyQhW3wF_QfOP2/view},
  abbr={Preprint},
  selected={true},
  preview={slowmo.jpg}
}

@misc{rajib2024fedctta,
  title={FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning},
  author={Rakibul Rajib, MD Akil Raihan Iftee, Mir Sazzat Hossain, A K M Mahbubur, Sajib Mistry, M Ashraful, and Amin Ahsan Ali},
  booktitle={},
  pages={},
  year={2024},
  publisher={Springer},
  abstract={Deep learning models often face performance degradation due to distribution shifts between training and testing data. Test-Time Adaptation (TTA) has emerged as a solution, allowing models to adapt dynamically during deployment without access to source data, making it suitable for privacy-sensitive domains. In federated learning scenar- ios, where clients operate on decentralized and heterogeneous data, per- forming TTA is particularly challenging due to the dynamic nature of data distributions and the need for privacy preservation. To address these challenges, we propose Federated Continual Test-Time Adapta- tion (FedCTTA), a new framework using teacher student based knowl- edge distillation that leverages batch normalization (BN) layer updates and similarity-based collaboration among clients to enable efficient and privacy-preserving adaptation. Our approach significantly reduces band- width usage by limiting parameter sharing to BN layers and enhances model accuracy in non-stationary environments through dynamic and collaborative adaptation. Experimental results on benchmark datasets demonstrate that FedCTTA achieves superior performance compared to state-of-the-art methods, particularly in scenarios involving tempo- ral and spatial heterogeneity. This work highlights the potential of com- bining TTA with federated learning to address real-world challenges in privacy-sensitive and dynamic environments.},
  pdf={https://drive.google.com/file/d/1PNsbo2Et7AzTXZQH5ZFUhVvMMK9aFp0X/view},
  abbr={Preprint},
  preview={fedctta.jpg}
}

@inproceedings{iftee2024moe-tta,
  title={MoE-TTA: Enhancing Continual Test-Time Adaptation for Vision-Language Models through Mixture of Experts},
  author={Iftee, Md Akil Raihan and Mahjabin, Wahida and Ekka, Anik and Das, Sunanda},
  booktitle={2024 27th International Conference on Computer and Information Technology (ICCIT)},
  pages={1-6},
  year={2024},
  publisher={IEEE},
  abstract={Continual learning enables vision-language models to incrementally acquire new knowledge without relying on access to the entire historical dataset. This capability is crucial for adapting to evolving data distributions in real-world scenar- ios, where models must handle domain shifts and incorporate new information while retaining previously learned knowledge. However, maintaining performance in large-scale models remains challenging due to parameter shifts during learning and the sig- nificant computational costs associated with full-model updates. To address these challenges, this paper introduces a novel method for Continual Test-Time Domain Adaptation (CTTDA) on vision- language datasets, leveraging a MoE and adapter modules to optimize domain-specific adaptation while maintaining zero-shot classification capabilities. Utilising LoRA in MoE framework within transformer layers, the model efficiently updates a small set of parameters by dynamically selecting experts. This approach minimizes computational costs by activating only a portion of the model, avoiding the need for full-model updates during domain adaptation. During test-time adaptation, entropy loss is calculated without access to labels, improving the model’s fine-tuning and guiding towards confident predictions across domains. A contrastive warm-up phase further optimizes the adapter blocks by enhancing the differentiation of domain-specific and domain- invariant features, thereby establishing a strong foundation for effective test-time adaptation. The proposed MoE-TTA model achieves an average accuracy of 36.43% across diverse ImageNet datasets and 32.93% in fine-grained classification, demonstrating promising results, especially in datasets like EuroSAT (51.67%), while being lower than several competitors, with no doubt in its ability to capture domain shifts effectively.},
  pdf={moe-tta.pdf},
  selected={true},
  abbr={Accepted},
  preview={moe.jpg}
}

@inproceedings{iftee2024organ-seg,
  title={ Organ-Seg: A Vision-Language and LLM-Enhanced Framework for User-Guided Abdominal Organ Image Segmentation},
  author={Iftee, Md Akil Raihan and Faija, Tasfia and Shoumya, Partho Choudhury and Das, Sunanda},
  booktitle={2024 27th International Conference on Computer and Information Technology (ICCIT)},
  pages={1-6},
  year={2024},
  publisher={IEEE},
  abstract={Medical image segmentation plays a vital role in diagnostic and treatment planning, where precision is crucial for accurate outcomes. Traditional segmentation methods, while effective in many areas, often fail to incorporate user-driven guidance, leading to errors in region identification, especially when irrelevant regions are segmented. In this study, we present a new, instruction-based medical image segmentation framework that enhances user interaction while delivering precise and context-aware results. Our approach addresses the limitations of previous works, such as vision-large language models (LLM) like LLaVA, which provide context but do not perform segmentation, and the Segment Anything Model, which performs segmenta- tion but does not incorporate user’s text-guided instruction. We propose a segmentation model framework that combines vision-language embeddings from LLava with SAM to perform accurate, query-based segmentation of medical images. A key innovation of our model framework is its ability to handle false premises—situations where a user queries for an organ not present in the image—by employing a similarity-based mechanism that prevents incorrect segmentation. Tested on MRI datasets, FLARE22, our system achieves the highest segmentation dice coefficient 63.9%, with significantly improved relevance and reliability. The results demonstrate the effectiveness of our approach in refining segmentation quality and enhancing user- guided interaction, thus offering an advanced tool for medical imaging applications.},
  pdf={organ-seg.pdf},
  selected={true},
  abbr={Accepted},
  preview={llavaseg.jpg}
}

@inproceedings{iftee2024next-gen,
  title={Next-Gen Heart Disease Prediction: A Gradient Boosting Approach with L1-Regularized Neural Networks},
  author={Iftee, Md Akil Raihan and Ashraf, Raiyan and Islam, Atiqul and Akter, Shahana and Das, Sunanda},
  booktitle={2024 27th International Conference on Computer and Information Technology (ICCIT)},
  pages={1-6},
  year={2024},
  publisher={IEEE},
  abstract={Medical image segmentation plays a vital role in diagnostic and treatment planning, where precision is crucial for accurate outcomes. Traditional segmentation methods, while effective in many areas, often fail to incorporate user-driven guidance, leading to errors in region identification, especially when irrelevant regions are segmented. In this study, we present a new, instruction-based medical image segmentation framework that enhances user interaction while delivering precise and context-aware results. Our approach addresses the limitations of previous works, such as vision-large language models (LLM) like LLaVA, which provide context but do not perform segmentation, and the Segment Anything Model, which performs segmenta- tion but does not incorporate user’s text-guided instruction. We propose a segmentation model framework that combines vision-language embeddings from LLava with SAM to perform accurate, query-based segmentation of medical images. A key innovation of our model framework is its ability to handle false premises—situations where a user queries for an organ not present in the image—by employing a similarity-based mechanism that prevents incorrect segmentation. Tested on MRI datasets, FLARE22, our system achieves the highest segmentation dice coefficient 63.9%, with significantly improved relevance and reliability. The results demonstrate the effectiveness of our approach in refining segmentation quality and enhancing user- guided interaction, thus offering an advanced tool for medical imaging applications.},
  pdf={next-gen_.pdf},
  abbr={Accepted},
  preview={nextgen.png}
}
